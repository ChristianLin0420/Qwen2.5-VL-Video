  0%|                                                                                                                                                             | 0/5 [00:00<?, ?it/s]/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/workspace/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 178, in <module>
    train(attn_implementation="flash_attention_2")
  File "/workspace/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 168, in train
    trainer.train()
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/accelerate/accelerator.py", line 2321, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 275, in backward
    self.engine.step()
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2322, in step
    self._take_model_step(lr_kwargs)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2225, in _take_model_step
    self.optimizer.step()
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2120, in step
    self._optimizer_step(sub_group_id)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 987, in _optimizer_step
    self.optimizer.step()
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/adamw.py", line 232, in step
    has_complex = self._init_group(
  File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/adamw.py", line 175, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 44.35 GiB of which 2.16 GiB is free. Process 228869 has 42.17 GiB memory in use. Of the allocated memory 39.55 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 178, in <module>
[rank0]:     train(attn_implementation="flash_attention_2")
[rank0]:   File "/workspace/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 168, in train
[rank0]:     trainer.train()
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/accelerate/accelerator.py", line 2321, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 275, in backward
[rank0]:     self.engine.step()
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2322, in step
[rank0]:     self._take_model_step(lr_kwargs)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2225, in _take_model_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2120, in step
[rank0]:     self._optimizer_step(sub_group_id)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 987, in _optimizer_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 140, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/adamw.py", line 232, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/opt/conda/envs/qwen/lib/python3.10/site-packages/torch/optim/adamw.py", line 175, in _init_group
[rank0]:     state["exp_avg_sq"] = torch.zeros_like(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 44.35 GiB of which 2.16 GiB is free. Process 228869 has 42.17 GiB memory in use. Of the allocated memory 39.55 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
